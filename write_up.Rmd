---
title: "Prediction Assignment Writeup"
author: "Abhishek Umrawal"
output: html_document
---

#1. Introduction

The project involves solving a *Classification Problem* (Predicting the Class based on some Independent Variables.)


We have been provided the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

The problem is approached through the following the three steps: 

1.1 Pre-Processing of Data (Discussing the Approach I have taken.)

1.2 Learning the Clasification Hypothesis using the Training Data

1.3 Using the Learned Model for Predicting Classification for the Testing Data


#2. Pre-Processing of Data

We have first identified the variables which are present in the testing data and then removed all those variables from the training data as well which are not in the testing data, as the final prediction is to be done for the testing data.

Also there was a categorical variable named as *new_window* whihch takes values either *yes* or *no*, we have recoded them as *1* and *0* respectively.


#3. Learning the Clasification Hypothesis using the Training Data

We need to learn a Classification hypothesis using the training data. Several algorithms like Classification Tree, Random Forests, ADA Boosting etc. have been introduced in the lectures. We are making use of the Classification Tree Algorithm for this purpose. 

Learning a Classification Tree for the Training Data:

```{r}
#Reading Training and Test Data.
training<-read.csv("pml-training.csv",header=TRUE)
testing<-read.csv("pml-testing.csv",header=TRUE)

#Including the Necessary Library rpart
library(rpart)

#Fitting the Model
fit <- rpart(classe ~.,method="class", data=training)
```
```{r,eval=FALSE}
# Displaying the Model Results
printcp(fit)

# Visualizing Cross-Validation Results
plotcp(fit)  

# Detailed Summary of Splits
summary(fit) 
```

## Assessing the Performance of Model on the Training Data

##3.1. Confusion Matrix

```{r,echo=FALSE}
pred_training<-predict(fit,training)
write.csv(pred_training,"pred_training.csv")
ObsVsPred_training<-read.csv("ObsVsPred_training.csv",header=TRUE)
library(gmodels)
Observed_Class<-ObsVsPred_training[,1]
Predicted_Class<-ObsVsPred_training[,2]
CrossTable(Observed_Class,Predicted_Class,prop.r=FALSE,prop.c=FALSE,prop.t=FALSE,prop.chisq=FALSE,format="SPSS")
```

##3.2 Visualizing Cross-Validation Results and Expected Out of Sample Error

Cross-Validation Results plot is as follows:

```{r,echo=FALSE}
# Visualizing Cross-Validation Results
plotcp(fit)  
```

We use the **Percentage of Misclassification in Cross-Validation** as an estimate of expected out of sample error. The percentage of misclassification turns out to be 26.02%. So the model can be taken as of good adequacy.


#4. Using the Learned Model for Predicting Classification for the Testing Data

Now we make use of the developed classification model to predict the classes for the testing data. The testing data has 20 observations and we need to predict the class of each of the observations.

Using the learned model we obtain the probabilities for each observation belonging to each of the 5 classes viz. A, B, C, D and E. The class for which an observation has the maximum probability we classify that observation into that class.

We get the following table representing individual probabilities and the predicted classes for each of the observations in the testing data.:

```{r,echo=FALSE}
pred_testing<-predict(fit,testing)
write.csv(pred_testing,"pred_testing.csv")
pred_testing<-read.csv("pred_testing2.csv",header=TRUE)
pred_testing
```

#5. Conclusion

The classification model learned using the training data works decently well for both training and testing sets indicating the adequacy of fitting strength and predictive strength of the model.

However other classification algorithms like Random Forest, ADA Boost etc. can also be utilized for the same problem.
