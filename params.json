{"name":"Prediction Assignment - Writeup","tagline":"This GitHub repository corresponds to the Course Project of the Practical Machine Learning course of the Data Science Specialization offered by Bloomberg School of Public Health of the Johns Hopkins University through Coursera.","body":"---\r\ntitle: \"Prediction Assignment Writeup\"\r\nauthor: \"Abhishek Umrawal\"\r\noutput: html_document\r\n---\r\n\r\n#1. Introduction\r\n\r\nThe project involves solving a *Classification Problem* (Predicting the Class based on some Independent Variables.)\r\n\r\n\r\nWe have been provided the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. \r\n\r\nMore information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).\r\n\r\nThe problem is approached through the following the three steps: \r\n\r\n1.1 Pre-Processing of Data (Discussing the Approach I have taken.)\r\n\r\n1.2 Learning the Clasification Hypothesis using the Training Data\r\n\r\n1.3 Using the Learned Model for Predicting Classification for the Testing Data\r\n\r\n\r\n#2. Pre-Processing of Data\r\n\r\nWe have first identified the variables which are present in the testing data and then removed all those variables from the training data as well which are not in the testing data, as the final prediction is to be done for the testing data.\r\n\r\nAlso there was a categorical variable named as *new_window* whihch takes values either *yes* or *no*, we have recoded them as *1* and *0* respectively.\r\n\r\n\r\n#3. Learning the Clasification Hypothesis using the Training Data\r\n\r\nWe need to learn a Classification hypothesis using the training data. Several algorithms like Classification Tree, Random Forests, ADA Boosting etc. have been introduced in the lectures. We are making use of the Classification Tree Algorithm for this purpose. \r\n\r\nLearning a Classification Tree for the Training Data:\r\n\r\n\r\n```r\r\n#Reading Training and Test Data.\r\ntraining<-read.csv(\"pml-training.csv\",header=TRUE)\r\ntesting<-read.csv(\"pml-testing.csv\",header=TRUE)\r\n\r\n#Including the Necessary Library rpart\r\nlibrary(rpart)\r\n\r\n#Fitting the Model\r\nfit <- rpart(classe ~.,method=\"class\", data=training)\r\n```\r\n\r\n```r\r\n# Displaying the Model Results\r\nprintcp(fit)\r\n\r\n# Visualizing Cross-Validation Results\r\nplotcp(fit)  \r\n\r\n# Detailed Summary of Splits\r\nsummary(fit) \r\n```\r\n\r\n## Assessing the Performance of Model on the Training Data\r\n\r\n##3.1. Confusion Matrix\r\n\r\n\r\n```\r\n## \r\n##    Cell Contents\r\n## |-------------------------|\r\n## |                   Count |\r\n## |-------------------------|\r\n## \r\n## Total Observations in Table:  19622 \r\n## \r\n##                | Predicted_Class \r\n## Observed_Class |        A  |        B  |        C  |        D  |        E  | Row Total | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n##              A |     4970  |      636  |      128  |      201  |      179  |     6114  | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n##              B |      175  |     2336  |      243  |      279  |      398  |     3431  | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n##              C |       59  |      244  |     2772  |      469  |      288  |     3832  | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n##              D |      299  |      461  |      189  |     2120  |      424  |     3493  | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n##              E |       77  |      120  |       90  |      147  |     2318  |     2752  | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n##   Column Total |     5580  |     3797  |     3422  |     3216  |     3607  |    19622  | \r\n## ---------------|-----------|-----------|-----------|-----------|-----------|-----------|\r\n## \r\n## \r\n```\r\n\r\n##3.2 Visualizing Cross-Validation Results and Expected Out of Sample Error\r\n\r\nCross-Validation Results plot is as follows:\r\n\r\n![plot of chunk unnamed-chunk-4](figure/unnamed-chunk-4.png) \r\n\r\nWe use the **Percentage of Misclassification in Cross-Validation** as an estimate of expected out of sample error. The percentage of misclassification turns out to be 26.02%. So the model can be taken as of good adequacy.\r\n\r\n\r\n#4. Using the Learned Model for Predicting Classification for the Testing Data\r\n\r\nNow we make use of the developed classification model to predict the classes for the testing data. The testing data has 20 observations and we need to predict the class of each of the observations.\r\n\r\nUsing the learned model we obtain the probabilities for each observation belonging to each of the 5 classes viz. A, B, C, D and E. The class for which an observation has the maximum probability we classify that observation into that class.\r\n\r\nWe get the following table representing individual probabilities and the predicted classes for each of the observations in the testing data.:\r\n\r\n\r\n```\r\n##    Obs Prob.A.   Prob.B. Prob.C. Prob.D.  Prob.E. Predicted.Class\r\n## 1    1 0.09884 0.5465116 0.00000 0.00000 0.354651               B\r\n## 2    2 0.77892 0.1745574 0.01359 0.02676 0.006175               A\r\n## 3    3 0.01396 0.5340314 0.22862 0.10122 0.122164               B\r\n## 4    4 0.83483 0.0007508 0.00000 0.06156 0.102853               A\r\n## 5    5 0.83483 0.0007508 0.00000 0.06156 0.102853               A\r\n## 6    6 0.02394 0.0819473 0.11684 0.17769 0.599594               E\r\n## 7    7 0.06650 0.1240409 0.03581 0.69693 0.076726               D\r\n## 8    8 0.12406 0.5243596 0.07986 0.12657 0.145153               B\r\n## 9    9 0.99366 0.0063371 0.00000 0.00000 0.000000               A\r\n## 10  10 0.77892 0.1745574 0.01359 0.02676 0.006175               A\r\n## 11  11 0.02394 0.5995943 0.08195 0.17769 0.116836               B\r\n## 12  12 0.01396 0.2286213 0.53403 0.10122 0.122164               C\r\n## 13  13 0.02394 0.5995943 0.08195 0.17769 0.116836               B\r\n## 14  14 0.99366 0.0063371 0.00000 0.00000 0.000000               A\r\n## 15  15 0.07636 0.1454545 0.10909 0.17818 0.490909               E\r\n## 16  16 0.12406 0.1265696 0.52436 0.07986 0.145153               B\r\n## 17  17 0.83483 0.0007508 0.00000 0.06156 0.102853               A\r\n## 18  18 0.10744 0.4008264 0.01033 0.33058 0.150826               B\r\n## 19  19 0.10744 0.4008264 0.01033 0.33058 0.150826               B\r\n## 20  20 0.01396 0.5340314 0.22862 0.10122 0.122164               B\r\n```\r\n\r\n#5. Conclusion\r\n\r\nThe classification model learned using the training data works decently well for both training and testing sets indicating the adequacy of fitting strength and predictive strength of the model.\r\n\r\nHowever other classification algorithms like Random Forest, ADA Boost etc. can also be utilized for the same problem.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}